{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Docs\n",
    "* [Spark Programming Guide](http://spark.apache.org/docs/1.5.1/programming-guide.html)\n",
    "* [API Docs](http://spark.apache.org/docs/1.5.1/api/python/)\n",
    "\n",
    "**Make sure you use are reading the docs for the correct version!**\n",
    "\n",
    "The version installed in the VM is Spark 1.5.1\n",
    "\n",
    "You will need to use various transformations and actions for RDDs, as well as several classes in the MLlib package.\n",
    "\n",
    "* [RDD functions](http://spark.apache.org/docs/1.5.1/api/python/pyspark.html#pyspark.RDD)\n",
    "* [MLlib Guide](http://spark.apache.org/docs/1.5.1/mllib-guide.html)\n",
    "* [MLlib API](http://spark.apache.org/docs/1.5.1/api/python/pyspark.mllib.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Parse data file\n",
    "\n",
    "We will be using a partially cleaned subset of the [Sentiment140](http://help.sentiment140.com/for-students) dataset created by Twitter.\n",
    "### 1a) load file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "linesRDD = sc.textFile('tweets.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b) How many instances are in the file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "<FILL IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1c) View a few lines to see the format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "<FILL IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1d) Split the input lines into two fields: sentiment class and text\n",
    "* 1.0 => positive sentiment\n",
    "* 0.0 => negative sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rawTweetsRDD = linesRDD.map(lambda x: str(x).split('\\t', 1)) # str() converts to ASCII\n",
    "rawTweetsRDD.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1e) What is the class distribution?\n",
    "Notice that each element in the RDD is a tuple of (class, text).\n",
    "\n",
    "You will want to count instances based on the first element, the class value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rawTweetsRDD.countByKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Clean data\n",
    "### 2a) Function for cleaning tweets\n",
    "To accurately model each input tweet, we need to remove information that is not necessary, and clean the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "    \"\"\" Clean tweet by removing URLs, mentions, punctation, and converting to lowercase\n",
    "    Args: \n",
    "        tweet (str): a single tweet\n",
    "    Returns:\n",
    "        str: lowercase tweet with URLs, mentions, and punctuation removed\n",
    "    \"\"\"\n",
    "    if len(tweet) > 0:\n",
    "        # remove links\n",
    "        tweet = re.sub('((www\\.[\\s]+)|(https?://[^\\s]+))','',tweet) \n",
    "        # remove mentions (i.e. @user)\n",
    "        tweet = re.sub('@[^\\s]+','',tweet)\n",
    "        specials = '\\'\"?,.!@#$%^&*\\(\\):;_~`-+=/'\n",
    "        trans = string.maketrans(specials, u' '*len(specials))\n",
    "        tweet = tweet.translate(trans)\n",
    "        # reduce multiple spaces\n",
    "        tweet = re.sub('[\\s]+', ' ', tweet)\n",
    "        # remove trailing spaces and convert to lowercase\n",
    "        return tweet.strip().lower()\n",
    "    return None\n",
    "\n",
    "# grab the first tweet's text just to test the function\n",
    "sampleTweet = rawTweetsRDD.take(1)[0][1]\n",
    "print sampleTweet\n",
    "print clean_tweet(sampleTweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b) Clean all tweets\n",
    "Apply the `clean_tweet()` function to all tweets via the RDD `map()` function.\n",
    "\n",
    "Remember each instance in the RDD is a tuple of (label, tweet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# clean all tweets\n",
    "cleanTweetsRDD = rawTweetsRDD.map(lambda (label,tweet): (float(label), clean_tweet(tweet)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2c) Verify clean tweets\n",
    "Verify that the `cleanTweetsRDD` is the same size as the `rawTweetsRDD` and print a sample of the clean tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "<FILL IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "<FILL IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Tokenize data\n",
    "Here, we remove stopwords to remove frequently used words, and only include words with 2 or more characters. \n",
    "### 3a) Load stopwords\n",
    "Load the `stopwords.txt` file into an RDD and use the `collect()` method to convert the RDD to a Python list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stopwordsList = sc.textFile('stopwords.txt').collect()\n",
    "print stopwordsList[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b) Tokenize function\n",
    "The `tokenize()` function defaults to removing stopwords and only including words with 2 or more characters. These parameters may be passed when calling the function to provide non-default behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stopwords = set(stopwordsList)\n",
    "\n",
    "def tokenize(text, minLength=2, remove_stopwords=True):\n",
    "    \"\"\" Tokenize a tweet, optionally remove stopwords and words less than a certain length\n",
    "    Args: \n",
    "        tweet (str): a single tweet\n",
    "    Returns:\n",
    "        str: tokenized tweet\n",
    "    \"\"\"\n",
    "    words = set([t for t in text.split(' ') if len(t) >= minLength])\n",
    "    if remove_stopwords:\n",
    "        return list(words - stopwords)\n",
    "    else:\n",
    "        return list(words)\n",
    "\n",
    "sampleTweet = cleanTweetsRDD.take(1)[0][1]\n",
    "print sampleTweet\n",
    "print tokenize(sampleTweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3c) Tokenize all tweets\n",
    "Apply the `tokenize()` function to all tweets in the `cleanTweetsRDD`. \n",
    "\n",
    "Remember each instance in the RDD is a tuple of (label, tweet).\n",
    "\n",
    "The resulting `tokenizedRDD` instances should be tuples of (label, list)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizedRDD = cleanTweetsRDD.map(lambda (label,tweet): (label, tokenize(tweet)))\n",
    "tokenizedRDD.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Create vocabulary\n",
    "To build a word vector-based model, we need to establish a vocabulary of most frequently used words to use as features for each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a) Wait! We need split the data! \n",
    "We can't build the vocabulary from our test set, that would be cheating!\n",
    "\n",
    "[`randomSplit()`](http://spark.apache.org/docs/1.4.1/api/python/pyspark.html#pyspark.RDD.randomSplit) function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainRDD, testRDD = tokenizedRDD.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4b) What is the size and class distribution of each dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'Training length: ', trainRDD.<FILL IN>\n",
    "print 'Training class distribution: ', (trainRDD.<FILL IN>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'Test length: ', testRDD.<FILL IN>\n",
    "print 'Test class distribution: ', (testRDD.<FILL IN>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4c) Build vocabulary\n",
    "Count the number of times each word occurs across all documents. We cache the RDD because we will be using it frequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from operator import add\n",
    "vocabulary = (trainRDD\n",
    "              .flatMap(lambda (label,words): words)\n",
    "              .map(lambda word: (word,1))\n",
    "              .reduceByKey(add)\n",
    "              .cache())\n",
    "vocabulary.cache()\n",
    "vocabulary.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4d) How many unique words are in the training data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocabulary.<FILL IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4e) What are the top 10 most frequently used words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocabulary.<FILL IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4f) Create inverted index for efficient lookup\n",
    "To create a word vector for each instance, we need to assign a feature ID to each word in the vocabulary.\n",
    "\n",
    "We specify the number of features (words) to use for the model. This will select to the top _n_ most frequently used words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_features = 5000\n",
    "features = (vocabulary\n",
    "            .sortBy(lambda (word,freq): -1*freq)\n",
    "            .map(lambda (word,freq): word)\n",
    "            .zipWithIndex()\n",
    "            .take(num_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to Python dict for fast lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "featureDict = dict(features)\n",
    "# convert to a string and print the first few chars, otherwise it would take a lot of screen space\n",
    "str(featureDict)[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Create word vector from text \n",
    "### 5a) Vectorize function\n",
    "Since each tweet contains a small amount of words, we should store each instance in a [SparseVector](https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.SparseVector).\n",
    "\n",
    "We use a [broadcast variable](http://spark.apache.org/docs/1.4.1/programming-guide.html#broadcast-variables) for communicating the feature list to the worker nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import SparseVector\n",
    "\n",
    "def vectorize(tokens, num_features):\n",
    "    \"\"\" Convert a tweet into a sparse binary word vector.\n",
    "    Args: \n",
    "        text (str): a single tweet\n",
    "    Returns:\n",
    "        SparseVector: sparse vector representation of the instance\n",
    "    \"\"\"\n",
    "    feat = features_broadcast.value\n",
    "    values = []\n",
    "    for t in tokens:\n",
    "        # check if word exists in the vocabulary\n",
    "        if t in feat:\n",
    "            values.append([feat[t], 1.0])\n",
    "    return SparseVector(num_features, values)\n",
    "\n",
    "# broadcast variable\n",
    "features_broadcast = sc.broadcast(featureDict)\n",
    "\n",
    "sampleTweet = trainRDD.take(1)[0][1]\n",
    "print sampleTweet\n",
    "vectorize(sampleTweet, len(featureDict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5b) Vectorize training and testing data\n",
    "We convert each instance to a [LabeledPoint](https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html?highlight=labeledpoint#pyspark.mllib.regression.LabeledPoint), which MLlib uses for storing instances with class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import LabeledPoint\n",
    "\n",
    "num_features = len(featureDict)\n",
    "trainVectorizedRDD = trainRDD.map(lambda (label,words): LabeledPoint(label, vectorize(words, num_features)))\n",
    "testVectorizedRDD = testRDD.map(lambda (label,words): LabeledPoint(label, vectorize(words, num_features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# preview the training RDD\n",
    "trainVectorizedRDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# preview the testing RDD\n",
    "testVectorizedRDD.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6) Build our model and predict!\n",
    "### 6a) Build Logistic Regression model\n",
    "For this model, we will use the [Logistic Regression model with SGD](https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html?highlight=logisticregression#pyspark.mllib.classification.LogisticRegressionWithSGD) implementation from MLlib.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
    "\n",
    "model = LogisticRegressionWithSGD.train(trainVectorizedRDD)\n",
    "# this will make the model spit out raw scores (probabilities) instead of class label\n",
    "model.clearThreshold()\n",
    "sampleInstance = testRDD.take(1)[0]\n",
    "sampleInstanceVectorized = testVectorizedRDD.take(1)[0]\n",
    "\n",
    "# sample instance and prediction\n",
    "print 'Sample instance text:', sampleInstance\n",
    "print 'Sample instance:', sampleInstanceVectorized\n",
    "print 'Prediction:', model.predict(sampleInstanceVectorized.features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6b) Evalute the model on our test data\n",
    "[BinaryClassificationMetrics](http://spark.apache.org/docs/1.5.1/api/python/pyspark.mllib.html?highlight=binary#pyspark.mllib.evaluation.BinaryClassificationMetrics) computes Area under ROC (AUC) and Area under PRC (Precision/Recall curve) from RDDs of (prediction, original_label)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "predictionAndLabel = testVectorizedRDD.map(lambda p : (float(model.predict(p.features)), p.label))\n",
    "auc = BinaryClassificationMetrics(predictionAndLabel).areaUnderROC\n",
    "print 'AUC:', auc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
